{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment Analysis of Climate Change Tweets\n",
    "\n",
    "Before getting at the sentiment analysis in tweets related to climate change let's see a simple example.\\\n",
    "Here is a classic problem from statistics class. Suppose you have two unmarked bowls of cookies. Bowl1 has 30 Oreos and 10 Chips Ahoy! The other bowl, Bowl2, contains 20 Oroes and 20 Chips Ahoy! So many questions follow from this simple setup.\n",
    "\n",
    "* If I do pick from bowl1 what are the chances that I’ll pick an Oreo (my favorite!)\n",
    "* If I pick from bowl2 what are the chances that the cookie is an Oreo?\n",
    "* If I pick from a random bowl and I get an Oreo what are the chances that I picked the cookie out of bowl1 versus bowl2\n",
    "\n",
    "Let's illustrate one of the most important mathematical formulas of the data science era – Bayes Theorem of conditional probability. We can state the theorem as follows:\n",
    "\n",
    "$$P\\left(A|B\\right) = \\frac{P\\left(B|A\\right)\\cdot P(A)}{P(B)}$$ \n",
    "\n",
    "We would say that the probability of A given B is the probability of B given A times the probability # of A divided by the probability of B.\n",
    "Luckily a lot of probability just boils down to couting and dividing. The spreadsheet below shows you how to use a table of data to calculate everything you need.\n",
    "\n",
    "| Cookie                | Bowl1   | Bowl2 | Total |\n",
    "|-----------------------|:-------:|:-----:|------:|\n",
    "| Oreo                  | 30      | 20    | 50    |\n",
    "| Chips Ahoy!           | 10      | 20    | 30    |\n",
    "| **Total**             | 40      | 40    | 80    |\n",
    "\n",
    "The probability table will be:\n",
    "\n",
    "| Probability           |         | Value |\n",
    "|-----------------------|---------|------:|\n",
    "| P(Bowl1)              | = 40/80 |  0.5  |\n",
    "| P(Bowl2)              | = 40/80 |  0.5  |\n",
    "| P(Oreo)               | = 50/80 | 0.625 |\n",
    "| P(ChipsAhoy!)         | = 30/80 | 0.375 |\n",
    "| P(Oreo \\| Bowl1)      | = 30/40 |  0.75 |\n",
    "| P(Oreo \\| Bowl2)      | = 20/40 |  0.5  |\n",
    "| P(ChipsAhoy! \\| Bowl1)| = 10/40 |  0.25 |\n",
    "| P(chipsAhoy! \\| Bowl2)| = 20/40 |  0.5  |\n",
    "| P(Bowl1 \\| Oreo)      | = 30/50 |  0.6  |\n",
    "| P(Bowl2 \\| Oreo)      | = 20/50 |  0.4  |\n",
    "| P(Bowl1 \\| ChipsAhoy!)| = 10/30 | 0.333 |\n",
    "| P(Bowl2 \\| ChipsAhoy!)| = 20/30 | 0.666 |\n",
    "\n",
    "Focusing on the problem of deciding if we chose from Bowl1 or Bowl2 for a moment. If we pick out an Oreo that means there is a 60% chance it came from Bowl1 and a 40% chance that it came from Bowl2. That doesn’t give us a ton of confidence that we have the right bowl. But what if we gather more data? What if we put the cookie back, carefully stir the cookies around and then pick another one. If this one comes out as an Oreo how can we use that information to improve our guess about which bowl we chose from?\n",
    "\n",
    "It turns out that it does, the more evidence we get the better we are able to predict the Bowl. When we go down this road we are going to take a bit of a mathematical shortcut so that our answer will not be a probability anymore, but thats OK as our end goal is to build a classifier that as an algorithm just given some data tells us whether something is one thing or another. For example given Oreo, Oreo, Oreo, Chips Ahoy! It is most likely that the bow we were picking from is Bowl1.\n",
    "\n",
    "The way to think about this is what is the probability of it being bowl one given Oreo, Oreo, Oreo, Chips Ahoy! Or to state it mathematically:\n",
    "\n",
    "$$P\\left(C|x_1,x_2,x_3...x_n\\right)$$ \n",
    "\n",
    "It turns out that this is proportional to:\n",
    "\n",
    "$$P\\left(x_1,x_2,x_3...x_n\\right|C)\\cdot P\\left(C\\right)$$ \n",
    "\n",
    "Now we can combine the individual probabilities using multiplication. So the above statement is again proportional to:\n",
    "\n",
    "$$P\\left(C_J\\right)\\cdot \\prod_{i}^n P\\left(x_i|C_j\\right)$$\n",
    "\n",
    "Now if we compute that formula for each possible Cj then the one with the higest value is our winner."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets work out the example we have outlined to get the scores given our Oreo, Oreo, Oreo, Chips Ahoy! example. The probability that we get an Oreo given that it is Bowl1 is 0.75 and the probability that it is a Chips Ahoy! given that it is Bowl1 is 0.25 The probability that a cookie comes from Bowl1 is 0.5 thus:\n",
    "\n",
    "$$Bowl1 = P(Bowl1)\\cdot P\\left(Oreo|Bowl1\\right)\\cdot P\\left(Oreo|Bowl1\\right)\\cdot P\\left(Oreo|Bowl1\\right)\n",
    "\\cdot P\\left(Ahoy!|Bowl1\\right)$$\n",
    "\n",
    "$$Bowl2 = P(Bowl2)\\cdot P\\left(Oreo|Bowl2\\right)\\cdot P\\left(Oreo|Bowl2\\right)\\cdot P\\left(Oreo|Bowl2\\right)\n",
    "\\cdot P\\left(Ahoy!|Bowl2\\right)$$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bowl1 = 0.053, Bowl2 = 0.031\n"
     ]
    }
   ],
   "source": [
    "Bowl1 = round((0.5 * 0.75 * 0.75 * 0.75 * 0.25), 3)\n",
    "Bowl2 = round((0.5 * 0.5 * 0.5 * 0.5 * 0.5), 3)\n",
    "\n",
    "print(f'Bowl1 = {Bowl1}, Bowl2 = {Bowl2}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Bowl1 value is higher than Bowl2 value, we can say that we were picking-up cookies out of Bowl1.\n",
    "\n",
    "If we modify the spreadsheet so that the number of chips ahoy in Bowl1 is 40, and the number of oreos in Bowl2 is 30, given our Oreo, Oreo, Oreo, Chips Ahoy! example. What are the new scores for Bowl1?\n",
    "\n",
    "| Cookie                | Bowl1   | Bowl2 | Total |\n",
    "|-----------------------|:-------:|:-----:|------:|\n",
    "| Oreo                  | 30      | 30    | 60    |\n",
    "| Chips Ahoy!           | 40      | 20    | 60    |\n",
    "| **Total**             | 70      | 50    | 120   |\n",
    "\n",
    "The probability table will be:\n",
    "\n",
    "| Probability           |         | Value |\n",
    "|-----------------------|---------|------:|\n",
    "| P(Bowl1)              |= 70/120 | 0.583 |\n",
    "| P(Bowl2)              |= 50/120 | 0.416 |\n",
    "| P(Oreo)               |= 60/120 |  0.5  |\n",
    "| P(ChipsAhoy!)         |= 60/120 |  0.5  |\n",
    "| P(Oreo \\| Bowl1)      | = 30/70 | 0.428 |\n",
    "| P(Oreo \\| Bowl2)      | = 30/50 |  0.6  |\n",
    "| P(ChipsAhoy! \\| Bowl1)| = 40/70 | 0.571 |\n",
    "| P(chipsAhoy! \\| Bowl2)| = 20/50 |  0.4  |\n",
    "| P(Bowl1 \\| Oreo)      | = 30/60 |  0.5  |\n",
    "| P(Bowl2 \\| Oreo)      | = 30/60 |  0.5  |\n",
    "| P(Bowl1 \\| ChipsAhoy!)| = 40/60 | 0.666 |\n",
    "| P(Bowl2 \\| ChipsAhoy!)| = 20/60 | 0.333 |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bowl1 = 0.026, Bowl2 = 0.036\n"
     ]
    }
   ],
   "source": [
    "Bowl1 = round((0.428 * 0.428 * 0.428 * 0.571 * 0.583), 3)\n",
    "Bowl2 = round((0.6 * 0.6 * 0.6 * 0.4 * 0.416), 3)\n",
    "\n",
    "print(f'Bowl1 = {Bowl1}, Bowl2 = {Bowl2}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Bowl2 value is higher than Bowl1 value, we can say that we were picking-up cookies out of Bowl2.\n",
    "\n",
    "Now lets add a third kind of cookie to both bowls. Suppose we had a bunch of Fig Newtons. 20 of them in Bow1 and 30 of them in Bowl2 and we have the following series of draws: Oreo, Fig Newton, Fig Newton, Chips Ahoy, Oreo. What are the new scores for Bowl1 and Bowl2?\n",
    "\n",
    "| Cookie                | Bowl1   | Bowl2 | Total |\n",
    "|-----------------------|:-------:|:-----:|------:|\n",
    "| Oreo                  | 30      | 30    | 60    |\n",
    "| Chips Ahoy!           | 40      | 20    | 60    |\n",
    "| Fig Newtons           | 20      | 30    | 50    |\n",
    "| **Total**             | 90      | 80    | 170   |\n",
    "\n",
    "The probability table will be:\n",
    "\n",
    "| Probability           |         | Value |\n",
    "|-----------------------|---------|------:|\n",
    "| P(Bowl1)              |= 90/170 | 0.529 |\n",
    "| P(Bowl2)              |= 80/170 | 0.471 |\n",
    "| P(Oreo)               |= 60/170 | 0.353 |\n",
    "| P(ChipsAhoy!)         |= 60/170 | 0.353 |\n",
    "| P(Fig Newtons)        |= 50/170 | 0.294 |\n",
    "| P(Oreo \\| Bowl1)      | = 30/90 | 0.333 |\n",
    "| P(Oreo \\| Bowl2)      | = 30/80 | 0.375 |\n",
    "| P(ChipsAhoy! \\| Bowl1)| = 40/90 | 0.444 |\n",
    "| P(chipsAhoy! \\| Bowl2)| = 20/80 | 0.25  |\n",
    "| P(Fig Newtons\\| Bowl1)| = 20/90 | 0.222 |\n",
    "| P(Fig Newtons\\| Bowl2)| = 30/80 | 0.375 |\n",
    "| P(Bowl1 \\| Oreo)      | = 30/60 |  0.5  |\n",
    "| P(Bowl2 \\| Oreo)      | = 30/60 |  0.5  |\n",
    "| P(Bowl1 \\| ChipsAhoy!)| = 40/60 | 0.666 |\n",
    "| P(Bowl2 \\| ChipsAhoy!)| = 20/60 | 0.333 |\n",
    "| P(Bowl1\\| Fig Newtons)| = 20/50 |  0.4  |\n",
    "| P(Bowl2\\| Fig Newtons)| = 30/50 |  0.6  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bowl1 = 0.001, Bowl2 = 0.006\n"
     ]
    }
   ],
   "source": [
    "Bowl1 = round((0.529 * 0.333 * 0.222 * 0.222 * 0.444* 0.333), 3)\n",
    "Bowl2 = round((0.471 * 0.375 * 0.375 * 0.375 * 0.25), 3)\n",
    "\n",
    "print(f'Bowl1 = {Bowl1}, Bowl2 = {Bowl2}')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since Bowl2 value is higher than Bowl1 value, we can say that we were picking-up cookies out of Bowl2."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Going from Cookies to Tweets\n",
    "\n",
    "This all gets much more interesting when we look at a more real world problem. In fact this kind of Bayesian Classification became extremely popular 20 years ago as the first spam filter for email that worked well. More recently it has become a good technique for doing sentiment analysis.\\\n",
    "Instead of bowls of cookies we have bags of words. One bag has all the words we have collected from millions of emails that users have marked as spam. The other bag contains all the words we have collected from emails that were not spam. We can build a table just like we did for our Oreo and Chips Ahoy example. Of course this will have a lot more rows as we have a much greater variety. Nevertheless we can count how many times each word occurs in our spam bag and how many times it occurs in the non-spam bag. And compute our probabilities from there.\\\n",
    "To start with, we have a bunch of tweets that have been categorized as either climate change is real, and tweets that are of the climate change is fake variety. We will use those to build our two bags of words. There are also a bunch of tweets that have categorized as neutral, but we will leave those apart and focus on the two extremes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tnum</th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3229</td>\n",
       "      <td>RT @TIME: Another blizzard: What happened to g...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4263</td>\n",
       "      <td>Another crooked scientist in global warming sc...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168</td>\n",
       "      <td>Ski resorts fight global warming|SALT LAKE CIT...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4708</td>\n",
       "      <td>D.C. Snowstorm: How Global Warming Makes Blizz...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3766</td>\n",
       "      <td>[@ClimateProgress] Energy and Global Warming N...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>3</td>\n",
       "      <td>Carbon offsets: How a Vatican forest failed to...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>4730</td>\n",
       "      <td>Enough with the \"Where's your global warming n...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>3765</td>\n",
       "      <td>Rich Galen: Is global warming another DC snow ...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>5596</td>\n",
       "      <td>Counting down to the World People's Conf on #C...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>4659</td>\n",
       "      <td>@BCBG25  It's the denial about alternative ene...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1565 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tnum                                              tweet existence\n",
       "0     3229  RT @TIME: Another blizzard: What happened to g...         N\n",
       "1     4263  Another crooked scientist in global warming sc...         N\n",
       "2      168  Ski resorts fight global warming|SALT LAKE CIT...         Y\n",
       "3     4708  D.C. Snowstorm: How Global Warming Makes Blizz...         Y\n",
       "4     3766  [@ClimateProgress] Energy and Global Warming N...         Y\n",
       "...    ...                                                ...       ...\n",
       "1560     3  Carbon offsets: How a Vatican forest failed to...         Y\n",
       "1561  4730  Enough with the \"Where's your global warming n...         Y\n",
       "1562  3765  Rich Galen: Is global warming another DC snow ...         N\n",
       "1563  5596  Counting down to the World People's Conf on #C...         Y\n",
       "1564  4659  @BCBG25  It's the denial about alternative ene...         Y\n",
       "\n",
       "[1565 rows x 3 columns]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "df_tweets = pd.read_csv('climate_tweets.csv')\n",
    "df_tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 Cleaning the Data\n",
    "Before 'dealing' with every tweet, cleaning is necessary. This is because some tweets have links and punctuation that would alternate a word at the moment to make the analysis. Finally, we'll lowercase every word, this is because the algorithm would be case-sensitive.\n",
    "\n",
    "1. Remove URLs and punctuation from tweet\n",
    "2. Convert all to lower case"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def removePunctuation(a_str:str)->str:\n",
    "    ''' Remove URL, punctuation and lowercase a str'''\n",
    "    new_str = re.sub(r'http:\\S*','',a_str) # Remove URL based on http:...\n",
    "    new_str = re.sub(r'[^\\w\\s]','',new_str).lower() # Remove punctuation and lowercasing the string\n",
    "    return new_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tnum</th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3229</td>\n",
       "      <td>rt time another blizzard what happened to glob...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4263</td>\n",
       "      <td>another crooked scientist in global warming sc...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168</td>\n",
       "      <td>ski resorts fight global warmingsalt lake city...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4708</td>\n",
       "      <td>dc snowstorm how global warming makes blizzard...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3766</td>\n",
       "      <td>climateprogress energy and global warming news...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1560</th>\n",
       "      <td>3</td>\n",
       "      <td>carbon offsets how a vatican forest failed to ...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1561</th>\n",
       "      <td>4730</td>\n",
       "      <td>enough with the wheres your global warming now...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1562</th>\n",
       "      <td>3765</td>\n",
       "      <td>rich galen is global warming another dc snow j...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1563</th>\n",
       "      <td>5596</td>\n",
       "      <td>counting down to the world peoples conf on cli...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1564</th>\n",
       "      <td>4659</td>\n",
       "      <td>bcbg25  its the denial about alternative energ...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1565 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      tnum                                              tweet existence\n",
       "0     3229  rt time another blizzard what happened to glob...         N\n",
       "1     4263  another crooked scientist in global warming sc...         N\n",
       "2      168  ski resorts fight global warmingsalt lake city...         Y\n",
       "3     4708  dc snowstorm how global warming makes blizzard...         Y\n",
       "4     3766  climateprogress energy and global warming news...         Y\n",
       "...    ...                                                ...       ...\n",
       "1560     3  carbon offsets how a vatican forest failed to ...         Y\n",
       "1561  4730  enough with the wheres your global warming now...         Y\n",
       "1562  3765  rich galen is global warming another dc snow j...         N\n",
       "1563  5596  counting down to the world peoples conf on cli...         Y\n",
       "1564  4659  bcbg25  its the denial about alternative energ...         Y\n",
       "\n",
       "[1565 rows x 3 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_tweets['tweet'] = df_tweets['tweet'].map(removePunctuation)\n",
    "# This could be done in a single line using maps and lambda function as:\n",
    "# df_tweets['tweet'] = (df_tweets['tweet'].map(lambda x: re.sub(r'http:\\S*','',x)\n",
    "#                                           .map(lambda x: re.sub(r'[^\\w\\s]','',x)).lower()\n",
    "df_tweets"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2 Building the Model\n",
    "1. Make a Dictionary for climate change existence and a Dictionary for climate change denial:\n",
    "    * For each tweet split the string into a list of words and add those words to the appropriate counter, based on the existence column. Do not include so called stop-words that is words that are popular and used in all tweets, such as a, an, the, etc.\n",
    "2. Make a dataframe that includes all of the words from both dictionaries where a word appears in both counters this dataframe should contain the total count.\n",
    "    * It should look like:\n",
    "\n",
    "<div align=\"center\">\n",
    "\n",
    "| Word    |  Y_counts  |  N_counts  | Total_count|\n",
    "|---------|:----------:|:----------:|-----------:|\n",
    "| Global  |    2271    |    2167    |    4438    |\n",
    "</div>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use:\\\n",
    "`df_tweets['existence'].unique()`\\\n",
    "we'll see that there are just two values 'Y' and 'N'.\n",
    "So let's create two series for each category using a boolean mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       rt time another blizzard what happened to glob...\n",
       "1       another crooked scientist in global warming sc...\n",
       "11      rt yidwithlid the ipccs latest climate change ...\n",
       "18      global warming in the hot seatby keith yost st...\n",
       "23      someone go tell the climate change crowd to go...\n",
       "                              ...                        \n",
       "1540    new_federalists  i have it on good auth tht gl...\n",
       "1551    rt keder    rt bglscout climate change weather...\n",
       "1552                  jmac82 so much for global warming p\n",
       "1558    climate change fraud  the scandal of solar pow...\n",
       "1562    rich galen is global warming another dc snow j...\n",
       "Name: tweet, Length: 323, dtype: object"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serie_denial = pd.Series(df_tweets['tweet'][df_tweets['existence'] == 'N'])\n",
    "serie_denial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2       ski resorts fight global warmingsalt lake city...\n",
       "3       dc snowstorm how global warming makes blizzard...\n",
       "4       climateprogress energy and global warming news...\n",
       "5       markey presses coal ceos on climate change den...\n",
       "6       glacial melt from global warming could unplug ...\n",
       "                              ...                        \n",
       "1559    los angeles jobs  ft work for greenpeace to st...\n",
       "1560    carbon offsets how a vatican forest failed to ...\n",
       "1561    enough with the wheres your global warming now...\n",
       "1563    counting down to the world peoples conf on cli...\n",
       "1564    bcbg25  its the denial about alternative energ...\n",
       "Name: tweet, Length: 1242, dtype: object"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serie_approval = pd.Series(df_tweets['tweet'][df_tweets['existence'] == 'Y'])\n",
    "serie_approval"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A stop word is a commonly used word (such as “the”, “a”, “an”, “in”) that a search engine has been programmed to ignore, both when indexing entries for searching and when retrieving them as the result of a search query.\\\n",
    "We would not want these words to take valuable processing time. For this, we can remove them easily, by storing a list of words that you consider to stop words. NLTK(Natural Language Toolkit) in python has a list of stopwords stored in 16 different languages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = stopwords.words('english')\n",
    "serie_denial = serie_denial.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "serie_approval = serie_approval.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we can create the dictionaries\n",
    "dict_denial = {}\n",
    "for tweet in serie_denial:\n",
    "    for word in tweet.split():\n",
    "        dict_denial[word] = dict_denial.get(word, 0) + 1\n",
    "\n",
    "dict_approval = {}\n",
    "for tweet in serie_approval:\n",
    "    for word in tweet.split():\n",
    "        dict_approval[word] = dict_approval.get(word, 0) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Y_counts</th>\n",
       "      <th>N_counts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ski</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resorts</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fight</td>\n",
       "      <td>48.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>global</td>\n",
       "      <td>648.0</td>\n",
       "      <td>268.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>warmingsalt</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4309</th>\n",
       "      <td>jmac82</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4310</th>\n",
       "      <td>spain</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4311</th>\n",
       "      <td>rich</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312</th>\n",
       "      <td>galen</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4313</th>\n",
       "      <td>job</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4314 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  Y_counts  N_counts\n",
       "0             ski       6.0       0.0\n",
       "1         resorts       6.0       0.0\n",
       "2           fight      48.0       0.0\n",
       "3          global     648.0     268.0\n",
       "4     warmingsalt       3.0       0.0\n",
       "...           ...       ...       ...\n",
       "4309       jmac82       0.0       1.0\n",
       "4310        spain       0.0       1.0\n",
       "4311         rich       0.0       1.0\n",
       "4312        galen       0.0       1.0\n",
       "4313          job       0.0       1.0\n",
       "\n",
       "[4314 rows x 3 columns]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The final DataFrame \"merging\" both of dictionaries - the NaN values must be replace with 0 value\n",
    "df_words = pd.DataFrame([dict_approval, dict_denial], index=['Y_counts', 'N_counts'])\n",
    "df_words = df_words.transpose().reset_index(names='word').fillna(0)\n",
    "df_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Laplace Smoothing\n",
    "If we take a look at the previous DataFrame we'll notice that there are some zero values, which is known as ***zero probability error***, and can be solve using Laplace smoothing, a technique for smoothing categorical data.\n",
    "A small-sample correction, or pseudo-count, will be incorporated in every probability estimate. Consequently, no probability will be zero. This is a way of regularizing Naive Bayes, and when the pseudo-count is zero.\n",
    "Thus, for a word ***w'*** that has a value of zero, could be represent as:\n",
    "\n",
    "$$P\\left(w'|Ycount\\right) = \\frac{\\text{number of tweets with } w' \\text{ and } y \\text{ = positive} +\\alpha}{N +\\alpha *K}$$\n",
    "\n",
    "***alpha*** represents the smoothing parameter, **K** represents the number of dimensions (features) in the data, and **N** represents the number of reviews with y=positive. If we choose a value of alpha != 0 (not equal to 0), the probability will no longer be zero even if a word is not present in the training dataset.\\\n",
    "As ***alpha*** increases, the likelihood probability moves towards uniform distribution (0.5). Most of the time, alpha = 1 is being used to remove the problem of zero probability.\\\n",
    "Sometimes Laplace smoothing technique is also known as “Add one smoothing”. In Laplace smoothing, 1 (one) is added to all the counts, and thereafter, the probability is calculated. This is one of the most trivial smoothing techniques out of all the techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word</th>\n",
       "      <th>Y_counts</th>\n",
       "      <th>N_counts</th>\n",
       "      <th>Total_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ski</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>resorts</td>\n",
       "      <td>7.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>fight</td>\n",
       "      <td>49.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>50.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>global</td>\n",
       "      <td>649.0</td>\n",
       "      <td>269.0</td>\n",
       "      <td>918.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>warmingsalt</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4309</th>\n",
       "      <td>jmac82</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4310</th>\n",
       "      <td>spain</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4311</th>\n",
       "      <td>rich</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4312</th>\n",
       "      <td>galen</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4313</th>\n",
       "      <td>job</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4314 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             word  Y_counts  N_counts  Total_count\n",
       "0             ski       7.0       1.0          8.0\n",
       "1         resorts       7.0       1.0          8.0\n",
       "2           fight      49.0       1.0         50.0\n",
       "3          global     649.0     269.0        918.0\n",
       "4     warmingsalt       4.0       1.0          5.0\n",
       "...           ...       ...       ...          ...\n",
       "4309       jmac82       1.0       2.0          3.0\n",
       "4310        spain       1.0       2.0          3.0\n",
       "4311         rich       1.0       2.0          3.0\n",
       "4312        galen       1.0       2.0          3.0\n",
       "4313          job       1.0       2.0          3.0\n",
       "\n",
       "[4314 rows x 4 columns]"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adding 1 to all the values in the dataframe\n",
    "df_words[['Y_counts', 'N_counts']] = df_words[['Y_counts', 'N_counts']].apply(lambda x: x+1)\n",
    "# Getting the total values for every word\n",
    "df_words['Total_count'] = df_words['Y_counts'] + df_words['N_counts']\n",
    "df_words"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we take a look at the dataframe above, we'll see that seems like our probability cookies tables, so now we can get the probability of any word coming from a denial or an approval tweet.\\\n",
    "Let's define a function that shows us information about the probabilities according to a word provided by a user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def whatChances(a_df:pd.DataFrame, a_str:str)->tuple:\n",
    "    '''Will return a tupple with the probabilities that a specified word comes from a denial or an approval tweet, \n",
    "    approval on position 0 and denial on position 1'''\n",
    "    appr_str = float(a_df['Y_counts'][a_df['word'] == a_str])\n",
    "    deni_str = float(a_df['N_counts'][a_df['word'] == a_str])\n",
    "    approval_words = a_df['Y_counts'].sum()\n",
    "    denial_words = a_df['N_counts'].sum()\n",
    "    appr = round((appr_str/approval_words), 5)\n",
    "    deni = round((deni_str/denial_words), 5)\n",
    "    return (appr,deni)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00011, 0.00025)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatChances(df_words, 'fake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.00208, 0.0005)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "whatChances(df_words, 'world')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classifying new Tweets\n",
    "Now let's figure it out how to classify a tweet using the Naive Bayes algorithm based and the previous dataframe as our training model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tnum</th>\n",
       "      <th>tweet</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1202</td>\n",
       "      <td>earthday aware consume waste treat place 1 see...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1100</td>\n",
       "      <td>government report says global warming may caus...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>580</td>\n",
       "      <td>cleaner air could speed global warming link</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2871</td>\n",
       "      <td>despite sceptics climate change must remain pr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>101</td>\n",
       "      <td>rt disturbedwater climate change increases hea...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>517</th>\n",
       "      <td>585</td>\n",
       "      <td>global warming today à_ blog archive à_ tackle...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>518</th>\n",
       "      <td>5379</td>\n",
       "      <td>icecovered volcanoes may answer climate change...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>519</th>\n",
       "      <td>3306</td>\n",
       "      <td>seriously libs really reaching ha rt drudge_re...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>520</th>\n",
       "      <td>4693</td>\n",
       "      <td>dear tcot rt newsongreen dc snowstorm global w...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>521</th>\n",
       "      <td>5272</td>\n",
       "      <td>irony deficient wapo faith frets climate chang...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>522 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     tnum                                              tweet\n",
       "0    1202  earthday aware consume waste treat place 1 see...\n",
       "1    1100  government report says global warming may caus...\n",
       "2     580        cleaner air could speed global warming link\n",
       "3    2871  despite sceptics climate change must remain pr...\n",
       "4     101  rt disturbedwater climate change increases hea...\n",
       "..    ...                                                ...\n",
       "517   585  global warming today à_ blog archive à_ tackle...\n",
       "518  5379  icecovered volcanoes may answer climate change...\n",
       "519  3306  seriously libs really reaching ha rt drudge_re...\n",
       "520  4693  dear tcot rt newsongreen dc snowstorm global w...\n",
       "521  5272  irony deficient wapo faith frets climate chang...\n",
       "\n",
       "[522 rows x 2 columns]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loading the dataset and removing punctuation, stop-words and lowercasing: \n",
    "df_climate = pd.read_csv('climate_test.csv')\n",
    "df_climate['tweet'] = df_climate['tweet'].map(removePunctuation)\n",
    "df_climate['tweet'] = df_climate['tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "df_climate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's re-write our previous function\n",
    "def existenceChances(a_str)->str:\n",
    "    '''Will clasify the tweet according to the words in there'''\n",
    "    yscore = 1.0\n",
    "    nscore = 1.0\n",
    "    for word in a_str.split():\n",
    "        if word in list(df_words['word']):\n",
    "            yscore = yscore * whatChances(df_words, word)[0]\n",
    "            nscore = nscore * whatChances(df_words, word)[1]\n",
    "        else: \n",
    "            continue\n",
    "    if yscore > nscore:\n",
    "        return 'Y'\n",
    "    else:\n",
    "        return 'N'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tnum</th>\n",
       "      <th>tweet</th>\n",
       "      <th>existence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>179</th>\n",
       "      <td>1270</td>\n",
       "      <td>pat mooney dangers geoengineering manipulating...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>5078</td>\n",
       "      <td>sec recognizes climate change material busines...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>337</th>\n",
       "      <td>1642</td>\n",
       "      <td>new_federalists good auth tht global warming a...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>864</td>\n",
       "      <td>soopermexican global warming clearly</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>290</th>\n",
       "      <td>4930</td>\n",
       "      <td>would beneficiaries global warming hype find h...</td>\n",
       "      <td>N</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>2184</td>\n",
       "      <td>grapes best earlywarning system effects climat...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1511</td>\n",
       "      <td>soaring mercury blame global warmingagartala a...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68</th>\n",
       "      <td>271</td>\n",
       "      <td>seasonal allergies getting worse climate chang...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>6028</td>\n",
       "      <td>dr_rose cali getting strange weather year call...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>471</th>\n",
       "      <td>1101</td>\n",
       "      <td>rt disturbedwater climate change increases hea...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>228</th>\n",
       "      <td>4596</td>\n",
       "      <td>murdochs fox news claim theres global warming ...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>390</td>\n",
       "      <td>protect wildlife habitat climate change link</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>542</td>\n",
       "      <td>uw biologist links early blooms global warming...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>3016</td>\n",
       "      <td>climate change dramatic impact hydropower tech...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>216</th>\n",
       "      <td>1264</td>\n",
       "      <td>rt qorianka eyes cochabamba alrdy suffering gl...</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     tnum                                              tweet existence\n",
       "179  1270  pat mooney dangers geoengineering manipulating...         Y\n",
       "181  5078  sec recognizes climate change material busines...         Y\n",
       "337  1642  new_federalists good auth tht global warming a...         N\n",
       "134   864               soopermexican global warming clearly         N\n",
       "290  4930  would beneficiaries global warming hype find h...         N\n",
       "21   2184  grapes best earlywarning system effects climat...         Y\n",
       "46   1511  soaring mercury blame global warmingagartala a...         Y\n",
       "68    271  seasonal allergies getting worse climate chang...         Y\n",
       "166  6028  dr_rose cali getting strange weather year call...         Y\n",
       "471  1101  rt disturbedwater climate change increases hea...         Y\n",
       "228  4596  murdochs fox news claim theres global warming ...         Y\n",
       "104   390       protect wildlife habitat climate change link         Y\n",
       "361   542  uw biologist links early blooms global warming...         Y\n",
       "20   3016  climate change dramatic impact hydropower tech...         Y\n",
       "216  1264  rt qorianka eyes cochabamba alrdy suffering gl...         Y"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_climate['existence'] = df_climate['tweet'].map(existenceChances)\n",
    "df_climate.sample(15)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "* This kind of sentiment analysis is quite 'simple' and works pretty well as we could see.\n",
    "* We must be careful in terms that not all the words are very well classified, and even using Laplace Smoothing, we should analyze if there are more words in our Bayesian model or words to 'smoothing'.\n",
    "* In machine learning, this kind of analysis is close to the supervised model. It is defined by its use of labeled datasets to train algorithms that classify data or predict outcomes accurately. As input data is fed into the model, it adjusts its weights until the model has been fitted appropriately, which occurs as part of the cross-validation process."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5238573367df39f7286bb46f9ff5f08f63a01a80960060ce41e3c79b190280fa"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
